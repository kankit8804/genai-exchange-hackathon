import os, json, uuid, re, logging
from datetime import datetime, timezone
from typing import List, Optional, Union

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends, Body
from firebase_admin import auth as fb_auth
from firebase_utils import get_firestore_client
from firebase_admin import firestore 
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
import os
from pydantic import BaseModel
from typing import Dict, Any
import traceability
import requests
import difflib

from dotenv import load_dotenv
load_dotenv()

# Vertex AI / BigQuery
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from google.cloud import bigquery

# Optional parsers for uploads
from pypdf import PdfReader
from docx import Document
from io import BytesIO

# -------------------- Config --------------------
PROJECT_ID = os.getenv("PROJECT_ID", "orbit-ai-472708")
LOCATION = os.getenv("LOCATION", "us-central1")
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-2.0-flash-001")
DATASET = os.getenv("DATASET", "orbit_ai_poc")
PROMPT_PATH = os.getenv("PROMPT_PATH", "prompts/prompt_poc_v1.txt")
PROMPT_VER = os.getenv("PROMPT_VERSION", "poc-v1")
CREATED_BY = os.getenv("CREATED_BY", "demo@orbit-ai")

TABLE_REQ = f"{PROJECT_ID}.{DATASET}.requirements"
TABLE_TC = f"{PROJECT_ID}.{DATASET}.generated_testcases"
TABLE_TRL = f"{PROJECT_ID}.{DATASET}.trace_links"

# Jira
JIRA_BASE = os.getenv("JIRA_BASE")
JIRA_USER = os.getenv("JIRA_USER")
JIRA_API_TOKEN = os.getenv("JIRA_API_TOKEN")
JIRA_PROJECT_KEY = os.getenv("JIRA_PROJECT_KEY")
JIRA_ISSUE_TYPE = os.getenv("JIRA_ISSUE_TYPE", "Task")

# -------------------- Lazy Clients --------------------
_bq = None
_vertex_ready = False

log = logging.getLogger("orbit-trace")
logging.basicConfig(level=logging.DEBUG)

def now_ts() -> str:
    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

def get_bq():
    global _bq
    if _bq is None:
        _bq = bigquery.Client(project=PROJECT_ID)
    return _bq

def ensure_vertex():
    global _vertex_ready
    if not _vertex_ready:
        vertexai.init(project=PROJECT_ID, location=LOCATION)
        _vertex_ready = True

# -------------------- FastAPI --------------------
app = FastAPI(title="Orbit AI Test Case Generator API", version="0.4")

# CORS allowlist via env (comma-separated exact origins) or a regex fallback that matches Cloud Run preview domains
allowed_origins_env = os.getenv("WEB_ALLOWED_ORIGINS", "").strip()
allowed_origin_regex_env = os.getenv("WEB_ALLOWED_ORIGIN_REGEX", "").strip()

# default: allow Cloud Run web previews and local dev
default_regex = r"^https://orbit-web-.*\.a\.run\.app$|^https://orbit-web-.*\.run\.app$|^http://(localhost|127\.0\.0\.1):3000$"

# Keep parsed allowlist handy for custom middleware
_origin_list = [o.strip() for o in allowed_origins_env.split(",") if o.strip()] if allowed_origins_env else []
_origin_pattern = re.compile(allowed_origin_regex_env or default_regex)

if _origin_list:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=_origin_list,
        allow_methods=["*"],
        allow_headers=["*"],
        allow_credentials=False,
        expose_headers=["*"],
        max_age=3600,
    )
else:
    app.add_middleware(
        CORSMiddleware,
        allow_origin_regex=allowed_origin_regex_env or default_regex,
        allow_methods=["*"],
        allow_headers=["*"],
        allow_credentials=False,
        expose_headers=["*"],
        max_age=3600,
    )


class EnsureCORSOnError(BaseHTTPMiddleware):
    """Ensure Access-Control-Allow-* headers are present even on 4xx/5xx.

    Some server-side exceptions can produce responses that miss CORS headers.
    This middleware adds the headers when the request Origin is allowed by
    either the explicit allowlist or the configured regex.
    """

    async def dispatch(self, request, call_next):
        origin = request.headers.get("origin")
        try:
            response = await call_next(request)
        except Exception as e:
            # Fallback JSON error with 500 while preserving CORS below
            from fastapi.responses import JSONResponse
            log.exception(f"Unhandled error: {e}")
            response = JSONResponse({"detail": "Internal Server Error"}, status_code=500)

        # Attach CORS headers if origin matches our policy
        if origin:
            allowed = (origin in _origin_list) or (_origin_pattern.match(origin) is not None)
            if allowed:
                # Mirror allowed origin; also set basic allow headers to unblock browsers
                response.headers.setdefault("Access-Control-Allow-Origin", origin)
                response.headers.setdefault("Vary", "Origin")
                response.headers.setdefault("Access-Control-Allow-Methods", "*")
                response.headers.setdefault("Access-Control-Allow-Headers", "*")
        return response


# Add as the outermost middleware so it wraps all responses
app.add_middleware(EnsureCORSOnError)

# -------------------- Helpers --------------------
def load_prompt() -> str:
    try:
        with open(PROMPT_PATH, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        log.warning(f"Prompt file not found at {PROMPT_PATH}: {e}")
        return (
            "You are an expert QA engineer.\n"
            "Requirement ID: {{req_id}}\n"
            "Requirement Text:\n{{requirement_text}}\n"
            "Return STRICT JSON with 'test_cases' array."
        )

def extract_excerpt(requirement_text: str, tc_title: str) -> str:
    """
    Extract the most relevant 1â€“2 sentence excerpt from the requirement text for a given test case title.
    Uses keyword and fuzzy matching to avoid returning the full document.
    """
    if not requirement_text.strip():
        return ""

    # Normalize input
    req = requirement_text.strip()
    title = (tc_title or "").strip()

    # Break into paragraphs or sentences
    chunks = re.split(r"(?<=[.?!])\s+|\n{2,}", req)
    chunks = [c.strip() for c in chunks if len(c.strip()) > 30]

    # Try fuzzy match to find the best-matching chunk
    best_chunk = ""
    best_score = 0.0
    for chunk in chunks:
        score = difflib.SequenceMatcher(None, title.lower(), chunk.lower()).ratio()
        if score > best_score:
            best_score = score
            best_chunk = chunk

    # If a strong match found, return that paragraph
    if best_chunk and best_score > 0.2:  # relaxed threshold
        log.debug(f"extract_excerpt fuzzy match (score={best_score:.2f}) len={len(best_chunk)}")
        return best_chunk[:600].strip()

    # Otherwise try word-based regex search
    title_words = [w for w in re.findall(r"\w+", title) if len(w) > 3]
    if title_words:
        pattern = "|".join(re.escape(w) for w in title_words)
        m = re.search(pattern, req, flags=re.IGNORECASE)
        if m:
            start = max(0, m.start() - 120)
            end = min(len(req), m.end() + 250)
            excerpt = req[start:end].strip()
            log.debug(f"extract_excerpt regex match len={len(excerpt)}")
            return excerpt

    # Fallback: first paragraph (limited to 400 chars)
    paras = [p.strip() for p in req.split("\n\n") if p.strip()]
    if paras:
        return paras[0][:400].strip()

    # Final fallback: first 300 chars
    return req[:300].strip()

def call_model(prompt: str) -> str:
    ensure_vertex()
    model = GenerativeModel(MODEL_NAME)
    cfg = GenerationConfig(temperature=0.2, max_output_tokens=2048)
    resp = model.generate_content(prompt, generation_config=cfg)
    if getattr(resp, "text", None):
        return resp.text
    parts = resp.candidates[0].content.parts if resp.candidates else []
    return "".join(getattr(p, "text", "") for p in parts)

def save_testcases(req_id: str, tcs: List[dict], text: str, project_id: Optional[str] = None) -> List[dict]:
    rows = []
    BASE_URL = "http://localhost:3000"
    for tc in tcs:
        # Extract excerpt before saving
        excerpt = tc.get("source_excerpt") or text[:300]
        rows.append({
            "test_id": tc.get("test_id") or "TEST-" + uuid.uuid4().hex[:8].upper(),
            "req_id": req_id,
            "title": tc.get("title") or "(no title)",
            "steps": tc.get("steps") or [],
            "expected_result": tc.get("expected_result") or "",
            "preconditions": tc.get("preconditions") or "",
            "severity": tc.get("severity") or "Medium",
            "compliance_tags": tc.get("compliance_tags") or [
                "IEC62304:SW_VER", "ISO13485:DocCtrl", "ISO27001:AccessCtrl"
            ],
            "trace_link": tc.get("trace_link") or f"{BASE_URL}/traceability/{req_id}",
            "source_excerpt": excerpt,
            "model_version": MODEL_NAME,
            "prompt_version": PROMPT_VER,
            "created_at": now_ts(),
            "created_by": CREATED_BY,
            "project_id": project_id or tc.get("project_id", ""),
        })

    errs = get_bq().insert_rows_json(TABLE_TC, rows)
    if errs:
        raise HTTPException(500, f"BigQuery insert errors: {errs}")
    return rows

def sniff_extract_text(filename: str, content: bytes) -> str:
    name = (filename or "").lower()
    if name.endswith(".pdf"):
        reader = PdfReader(BytesIO(content))
        return "\n\n".join(p.extract_text() or "" for p in reader.pages)
    if name.endswith(".docx"):
        doc = Document(BytesIO(content))
        return "\n".join(p.text for p in doc.paragraphs)
    return content.decode("utf-8", errors="ignore")

def upsert_requirement(req_id: str, title: str, text: str):
    row = [{
        "req_id": req_id,
        "source_type": "upload",
        "source_uri": f"upload://{req_id}",
        "title": title or "(Uploaded)",
        "text": text,
        "checksum": "",
        "created_at": now_ts(),
        "created_by": CREATED_BY
    }]
    errors = get_bq().insert_rows_json(TABLE_REQ, row)
    if errors:
        msg = " ".join(str(e) for e in errors)
        if "duplicate" not in msg.lower():
            raise HTTPException(500, f"Requirement upsert failed: {errors}")

# -------------------- Routes --------------------
app.include_router(traceability.router)

@app.get("/health")
def health():
    return {"ok": True, "model": MODEL_NAME, "bq": TABLE_TC}

@app.post("/generate")
def generate(body: dict):
    rid = (body.get("req_id") or f"REQ-{uuid.uuid4().hex[:6].upper()}").strip()
    text = body.get("text", "").strip()

    if not text:
        job = get_bq().query(
            f"SELECT text FROM `{TABLE_REQ}` WHERE req_id=@rid",
            job_config=bigquery.QueryJobConfig(
                query_parameters=[bigquery.ScalarQueryParameter("rid", "STRING", rid)]
            ),
        )
        row = next(iter(job.result()), None)
        if not row:
            raise HTTPException(404, f"Requirement {rid} not found")
        text = row["text"]

    prompt = load_prompt().replace("{{req_id}}", rid).replace("{{requirement_text}}", text)
    out = call_model(prompt)

    try:
        payload = json.loads(re.search(r"\{.*\}", out, re.DOTALL).group(0))
    except Exception:
        raise HTTPException(500, "Model did not return valid JSON")

    tcs = payload.get("test_cases", [])
    if not tcs:
        raise HTTPException(500, "No test_cases returned by model")

    saved = save_testcases(rid, tcs, text)
    return {"req_id": rid, "generated": len(saved), "test_cases": saved}

@app.post("/ingest")
async def ingest_requirement(file: UploadFile = File(...), title: Optional[str] = Form(None)):
    content = await file.read()
    text = sniff_extract_text(file.filename, content)
    rid = f"REQ-{uuid.uuid4().hex[:6].upper()}"
    upsert_requirement(rid, title or file.filename, text)

    prompt = load_prompt().replace("{{req_id}}", rid).replace("{{requirement_text}}", text)
    out = call_model(prompt)
    payload = json.loads(re.search(r"\{.*\}", out, re.DOTALL).group(0))
    tcs = payload.get("test_cases", [])

    saved = save_testcases(rid, tcs, text)
    return {"req_id": rid, "generated": len(saved), "test_cases": saved}
def fill_prompt(template: str, req_id: str, text: str, compliance: Optional[List[str]] = None) -> str:
    """
    Fill the template prompt with requirement details and optional compliance tags.
    """
    tip = ""
    if compliance:
        tip = "\nEmphasize compliance with: " + ", ".join(compliance) + \
              ". Reflect this in severity, steps, expected results, and tags."

    return (
        template
        .replace("{{req_id}}", req_id)
        .replace("{{requirement_text}}", text + tip)
    )

def extract_json(text: str) -> str:
    """
    Extract JSON object from model output.
    Removes Markdown fences (```json ... ```) and returns the first valid JSON block.
    """
    import re

    # Remove Markdown code fences if present
    cleaned = re.sub(r"^```(?:json)?|```$", "", text.strip(), flags=re.MULTILINE)

    # Extract the first JSON object using regex
    match = re.search(r"\{.*\}", cleaned, flags=re.DOTALL)
    return match.group(0) if match else cleaned


@app.post("/generate_unified")
async def generate_unified(
    files: Optional[List[UploadFile]] = None,
    links: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    req_id: Optional[str] = Form(None),
    title: Optional[str] = Form(None),
    project_id: Optional[str] = Form(None),
):
    """
    Unified endpoint for:
    - Multiple file uploads (PDF/DOCX/TXT/MD)
    - Multiple web links (as JSON array)
    - Free-text description
    - Optional project_id (links to user project history)
    - Existing req_id (re-generation)
    """
    extracted_texts = []
    source_type = "manual"

    # ---- Handle uploaded files ----
    if files:
        for file in files:
            content = await file.read()
            extracted_text = sniff_extract_text(file.filename, content)
            if extracted_text.strip():
                extracted_texts.append(extracted_text)
        source_type = "upload"

    # ---- Handle links ----
    if links:
        try:
            link_list = json.loads(links)
            for link in link_list:
                try:
                    resp = requests.get(link, timeout=10)
                    resp.raise_for_status()
                    raw = resp.text
                    cleaned = re.sub(r"<[^>]+>", "", raw)
                    if cleaned.strip():
                        extracted_texts.append(cleaned.strip())
                except Exception as e:
                    print(f"âš ï¸ Failed to read link {link}: {e}")
            source_type = "link"
        except Exception as e:
            raise HTTPException(400, f"Invalid links JSON: {e}")

    # ---- Handle free text ----
    if description and description.strip():
        extracted_texts.append(description.strip())
        source_type = "text"

    # ---- Validation ----
    if not extracted_texts:
        raise HTTPException(400, "No valid text provided from file, link, or description.")

    combined_text = "\n\n".join(extracted_texts)
    rid = (req_id or f"REQ-{uuid.uuid4().hex[:6].upper()}").strip()

    # ---- Upsert requirement ----
    upsert_requirement(rid, title or "(Unified Upload)", combined_text)

    # ---- Prepare LLM prompt ----
    prompt = fill_prompt(load_prompt(), rid, combined_text)
    out = call_model(prompt)

    try:
        payload = json.loads(extract_json(out))
    except json.JSONDecodeError as e:
        raise HTTPException(500, f"Model output invalid JSON: {e}")

    tcs = payload.get("test_cases", [])
    if not isinstance(tcs, list) or not tcs:
        raise HTTPException(500, "Model returned no test_cases")

    # ---- Extract focused excerpt ----
    def extract_excerpt(requirement_text: str, tc_title: str) -> str:
        if not requirement_text.strip():
            return ""
        words = [w for w in re.findall(r"\w+", tc_title) if len(w) > 3]
        if words:
            pattern = "|".join(re.escape(w) for w in words)
            m = re.search(pattern, requirement_text, flags=re.IGNORECASE)
            if m:
                start = max(0, m.start() - 150)
                end = min(len(requirement_text), m.end() + 200)
                return requirement_text[start:end].strip()
        # fallback to first paragraph
        paras = [p.strip() for p in requirement_text.split("\n\n") if p.strip()]
        return paras[0][:400] if paras else requirement_text[:300]

    for tc in tcs:
        tc["source_excerpt"] = extract_excerpt(combined_text, tc.get("title", ""))
        if project_id:
            tc["project_id"] = project_id  # link test case to project

    saved = save_testcases(rid, tcs, combined_text, project_id)

    return {
        "ok": True,
        "req_id": rid,
        "title": title or "(Unified Upload)",
        "project_id": project_id,
        "source_type": source_type,
        "generated": len(saved),
        "test_cases": saved,
        "project_id": project_id,
    }

@app.get("/testcases/project/{project_id}")
def get_testcases_by_project(project_id: str):
    """
    Fetch all generated test cases for a given project_id from BigQuery.
    """
    try:
        query = f"""
        SELECT 
            test_id, 
            req_id, 
            title, 
            severity, 
            expected_result, 
            steps, 
            created_at, 
            project_id,
            source_excerpt,
        FROM `{TABLE_TC}`
        WHERE project_id = @pid
        ORDER BY created_at DESC
        """
        job = get_bq().query(
            query,
            job_config=bigquery.QueryJobConfig(
                query_parameters=[bigquery.ScalarQueryParameter("pid", "STRING", project_id)]
            ),
        )

        results = []
        for row in job.result():
            results.append({
                "test_id": row["test_id"],
                "req_id": row["req_id"],
                "title": row["title"],
                "severity": row["severity"],
                "expected_result": row["expected_result"],
                "steps": row["steps"],
                "createdAt": row["created_at"],
                "project_id": row["project_id"],
                "source_excerpt": row["source_excerpt"],
            })

        return {"ok": True, "count": len(results), "test_cases": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching testcases: {e}")
    

class PushBody(BaseModel):
    summary: str
    steps: Optional[List[str]] = None
    test_id: str | None = None
    req_id: str | None = None
    jira_domain: str
    jira_email: str
    jira_api_token: str
    jira_project_key: str
    jira_issue_type: str = "Task"  # optional default


def build_adf_description(summary: str, steps: list[str] | None = None, expected: str | None = None):
    """
    Builds a valid Atlassian Document Format (ADF) JSON for Jira Cloud.
    Steps are rendered as an ordered (numbered) list.
    """
    adf = {
        "type": "doc",
        "version": 1,
        "content": []
    }

    adf["content"].append({
        "type": "paragraph",
        "content": [{"type": "text", "text": f"ðŸ§ª {summary or 'No summary provided'}"}]
    })

    if steps and isinstance(steps, list):
        adf["content"].append({
            "type": "orderedList",
            "content": [
                {
                    "type": "listItem",
                    "content": [
                        {
                            "type": "paragraph",
                            "content": [{"type": "text", "text": step}]
                        }
                    ]
                } for step in steps
            ]
        })

    if expected:
        adf["content"].append({
            "type": "paragraph",
            "content": [{"type": "text", "text": f" Expected Result: {expected}"}]
        })

    return adf

def update_is_pushed_by_test_id(test_ids: Union[str, List[str]], is_pushed: bool):
    """
    Update the Is_pushed column in BigQuery for given test_id(s).
    
    Args:
        test_ids: A single test_id string or a list of test_id strings.
        is_pushed: Boolean value to set for the Is_pushed column.
    """
    client = get_bq()
    table = TABLE_TC

    # Normalize to list
    if isinstance(test_ids, str):
        test_ids = [test_ids]

    if not test_ids:
        raise HTTPException(400, "No test_id(s) provided for update.")

    # Create safe IN clause
    test_id_list = ", ".join([f"'{tid}'" for tid in test_ids])

    query = f"""
        UPDATE `{table}`
        SET Is_pushed = @is_pushed
        WHERE test_id IN ({test_id_list})
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("is_pushed", "BOOL", is_pushed),
        ]
    )

    try:
        client.query(query, job_config=job_config).result()
    except Exception as e:
        raise HTTPException(500, f"BigQuery update failed: {e}")

    return {
        "updated_test_ids": test_ids,
        "is_pushed": is_pushed,
        "updated_count": len(test_ids),
    }

@app.post("/push/jira")
def push_jira(body: PushBody):
    log.debug(f"Push to Jira called with: {body}")
    if not (body.jira_domain and body.jira_email and body.jira_api_token and body.jira_project_key):
        raise HTTPException(400, "Missing Jira credentials from request body")

    jira_domain = body.jira_domain.replace("https://", "").replace("http://", "")

    adf = build_adf_description(
        summary=body.summary or f"Test Case {body.test_id}",
        steps=body.steps or None,
        expected=None
    )
    log.debug(f"Jira ADF: {adf}")
    url = f"https://{jira_domain}/rest/api/3/issue"
    payload = {
        "fields": {
            "project": {"key": body.jira_project_key},
            "summary": body.summary or f"Test Case {body.test_id}",
            "description": adf,
            "labels": ["orbit-ai", "test-case"],
            "issuetype": {"name": body.jira_issue_type}
        }
    }

    r = requests.post(
        url,
        json=payload,
        auth=(body.jira_email, body.jira_api_token),
        headers={"Accept": "application/json", "Content-Type": "application/json"}
    )

    if r.status_code not in (200, 201):
        raise HTTPException(500, f"Jira push failed: {r.text}")

    data = r.json()
    issue_key = data.get("key")
    issue_url = f"https://{jira_domain}/browse/{issue_key}"

    # update_is_pushed_by_test_id(body.test_id, True)

    return {"ok": True, "external_key": issue_key, "external_url": issue_url}

@app.post("/manual/testcase")
async def create_manual_testcase(body: dict):
    """
    Create a manual test case entry in BigQuery.
    Required fields: title, steps, expected_result, preconditions, severity, project_id, user_id
    """
    try:
        req_id = (body.get("req_id") or f"REQ-{uuid.uuid4().hex[:6].upper()}").strip()
        tc = {
            "test_id": "TEST-" + uuid.uuid4().hex[:8].upper(),
            "req_id": req_id,
            "title": body.get("title") or "(no title)",
            "steps": body.get("steps") or [],
            "expected_result": body.get("expected_result") or "",
            "preconditions": body.get("preconditions") or "",
            "severity": body.get("severity") or "Medium",
            "trace_link": "",
            "source_excerpt": "",
            "model_version": MODEL_NAME,
            "prompt_version": PROMPT_VER,
            "created_at": now_ts(),
            "created_by": body.get("user_id") or "unknown_user",
            "project_id": body.get("project_id") or "",
        }

        errs = get_bq().insert_rows_json(TABLE_TC, [tc])
        if errs:
            log.error(f"Manual test case insert error: {errs}")
            return {"ok": False, "error": str(errs)}

        return {"ok": True, "test_id": tc["test_id"], "req_id" : tc["req_id"], "createdAt": tc["created_at"]}

    except Exception as e:
        log.error(f"Manual test case creation failed: {e}")
        return {"ok": False, "error": str(e)}

@app.post("/manual/testcase/update")
async def update_manual_testcase(body: Dict[str, Any]):
    """
    Update a manual test case entry in BigQuery by test_id.
    """
    try:
        test_id = body.get("test_id")
        if not test_id:
            raise HTTPException(status_code=400, detail="Missing test_id")

        client = get_bq()
        table = TABLE_TC

        allowed_fields = ["title", "expected_result", "steps", "severity"]
        update_fields = {k: body.get(k) for k in allowed_fields if k in body}

        if not update_fields:
            raise HTTPException(status_code=400, detail="No fields to update")

        set_clauses = []
        query_params = [bigquery.ScalarQueryParameter("test_id", "STRING", test_id)]

        for k, v in update_fields.items():
            if k == "steps" and isinstance(v, list):
                set_clauses.append(f"{k} = @{k}")
                query_params.append(bigquery.ArrayQueryParameter(k, "STRING", v))
            else:
                set_clauses.append(f"{k} = @{k}")
                query_params.append(bigquery.ScalarQueryParameter(k, "STRING", v))

        set_clause_str = ", ".join(set_clauses)

        query = f"""
            UPDATE `{table}`
            SET {set_clause_str}
            WHERE test_id = @test_id
        """

        job_config = bigquery.QueryJobConfig(query_parameters=query_params)
        job = client.query(query, job_config=job_config)
        job.result()

        print("Testcase {test_id} updated successfully.")
        return {"ok": True, "test_id": test_id}

    except Exception as e:
        print("Update error:", e)
        return {"ok": False, "error": str(e)}
    
@app.post("/push/jira/bulk")
def push_jira_bulk(body: list[PushBody]):
    results = []
    for item in body:
        try:
            print(item)
            result = push_jira(item)  # reuse your existing function
            results.append({"test_id": item.test_id, "ok": True, "key": result["external_key"]})
        except HTTPException as e:
            results.append({"test_id": item.test_id, "ok": False, "error": str(e.detail)})
    return {"results": results}


@app.get("/projects/{project_id}/members")
def get_project_members(project_id: str):
    db = get_firestore_client()
    proj_ref = db.collection("projects").document(project_id)
    members_ref = proj_ref.collection("members")
    members = [m.to_dict() for m in members_ref.stream()]

    proj_doc = proj_ref.get()
    owner_email = None
    if proj_doc.exists:
        uid = proj_doc.to_dict().get("uid")
        try:
            owner_email = fb_auth.get_user(uid).email
        except Exception:
            owner_email = None

    if owner_email and not any(m["email"] == owner_email for m in members):
        members.insert(0, {"email": owner_email, "role": "owner"})
    return {"ok": True, "members": members}


@app.post("/projects/{project_id}/share")
def share_project(project_id: str, body: dict = Body(...)):
    """
    body = {"email": "invitee@example.com", "addedBy": "owner@example.com"}
    """
    email = body.get("email")
    added_by = body.get("addedBy")

    if not email:
        raise HTTPException(status_code=400, detail="Missing email")

    db = get_firestore_client()
    proj_ref = db.collection("projects").document(project_id)
    members_ref = proj_ref.collection("members")

    existing = [m.to_dict() for m in members_ref.stream()]
    if len(existing) >= 5:
        raise HTTPException(status_code=400, detail="Max 5 members allowed")
    if any(m.get("email") == email for m in existing):
        raise HTTPException(status_code=400, detail="User already added")

    member_doc = members_ref.document(email)
    member_doc.set({
        "email": email,
        "addedBy": added_by or None,
        "role": "member",
        "addedAt": firestore.SERVER_TIMESTAMP
    })

    return {"ok": True, "message": f"{email} added successfully"}